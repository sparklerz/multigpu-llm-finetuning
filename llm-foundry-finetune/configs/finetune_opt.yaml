# ---- experiment identity ----
run_name: opt_dolly_sft
seed: ${variables.global_seed}    # this is the recognized field

# ---- any free-form variables go here ----
variables:
  global_seed: 42
  grad_accum: 4

device_train_microbatch_size: 2
global_train_batch_size: 8
precision: amp_fp16
max_seq_len: 1024

# ---- model ----
model:
  name: hf_causal_lm
  pretrained_model_name_or_path: facebook/opt-1.3b
  trust_remote_code: false
  resize_embeddings: true

# ---- tokenizer ----
tokenizer:
  name: facebook/opt-1.3b
  pretrained_model_name_or_path: facebook/opt-1.3b
  init_device: cpu
  additional_special_tokens:
    - "<|end_of_prompt|>"
    - "<|response|>"
    - "<|end_of_response|>"

# ---- data ----
train_loader:
  name: finetuning
  dataset:
    hf_name: json
    split: train
    # Use the JSONL files generated by ``prepare_dolly.py`` which contain
    # ``prompt`` and ``response`` keys.
    hf_kwargs:
      data_files:
        train: data/dolly_15k_txt/train.jsonl
    decoder_only_format:
      instruction_key: prompt
      response_key: response
      train_on_prompt: false
      add_eos: true
    shuffle: true
    max_seq_len: 1024
  drop_last: false
  device_batch_size: 1
  num_workers: 2
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  timeout: 0

eval_loader:
  name: finetuning
  dataset:
    hf_name: json
    split: validation
    hf_kwargs:
      data_files:
        validation: data/dolly_15k_txt/validation.jsonl
    decoder_only_format:
      instruction_key: prompt
      response_key: response
      train_on_prompt: false
      add_eos: true
    shuffle: false
    max_seq_len: 1024
  drop_last: false
  device_batch_size: 1
  num_workers: 2
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  timeout: 0

# ---- optimisation ----
max_duration: 2ep
optimizer:
  name: decoupled_adamw
  lr: 1e-6
  betas: [0.9, 0.95]
  weight_decay: 0.0

algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0

scheduler:
  name: cosine_with_warmup
  t_warmup: 0.05ep
  t_max: 2ep
  alpha_f: 0.1
  scale_warmup: false

# ---- FSDP sharding ----
fsdp_config:
  sharding_strategy: FULL_SHARD        # shards model params, grads & opt-state
  mixed_precision: DEFAULT
  activation_checkpointing: true      # save memory by recomputing activations
  limit_all_gathers: true             # reduce all-gather overhead
  cpu_offload: false
  verbose: true                       # logs “Using FSDP FULL_SHARD”

# ---- logging / callbacks ----
loggers:
  wandb:
    project: llm-foundry-demo
    entity: kannansarat9
    log_grads: true 
callbacks:
  speed_monitor: {}
  lr_monitor: {}
  hf_checkpointer:
    save_folder: ./checkpoints/huggingface  # where to write HF-format checkpoints
    save_interval: 1ep                      # every epoch
    huggingface_folder_name: "ep{epoch}"    # sub-folder format
    precision: float16
    overwrite: true
    mlflow_registered_model_name: null      # no MLflow registry
    mlflow_logging_config: null             # defaults only
    flatten_imports: ["llmfoundry"]

eval_interval: 100ba
save_interval: 0          # disable checkpoints on Kaggle unless you need them
