# ---- experiment identity ----
run_name: bloom_dolly_sft
seed: ${variables.global_seed}    # this is the recognized field

# ---- any free-form variables go here ----
variables:
  global_seed: 42
  grad_accum: 4

device_train_microbatch_size: 1
global_train_batch_size: 8
precision: amp_fp16
max_seq_len: 1024

# ---- model ----
model:
  name: hf_causal_lm
  pretrained_model_name_or_path: bigscience/bloom-3b
  trust_remote_code: true

# ---- tokenizer ----
tokenizer:
  name: bigscience/bloom-3b
  pretrained_model_name_or_path: bigscience/bloom-3b
  init_device: cpu

# ---- data ----
train_loader:
  name: finetuning
  dataset:
    hf_name: json
    split: train
    # Use the JSONL files generated by ``prepare_dolly.py`` which contain
    # ``prompt`` and ``response`` keys.
    hf_kwargs:
      data_files:
        train: data/dolly_15k_txt/train.jsonl
    decoder_only_format: false
    shuffle: true
    max_seq_len: 1024
  drop_last: false
  device_batch_size: 1
  num_workers: 2
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  timeout: 0

eval_loader:
  name: finetuning
  dataset:
    hf_name: json
    split: validation
    hf_kwargs:
      data_files:
        validation: data/dolly_15k_txt/validation.jsonl
    decoder_only_format: false
    shuffle: false
    max_seq_len: 1024
  drop_last: false
  device_batch_size: 1
  num_workers: 2
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  timeout: 0

# ---- optimisation ----
max_duration: 2ep
optimizer:
  name: adamw_bnb_8bit
  lr: 1e-5
  betas: [0.9, 0.95]
  weight_decay: 0.01

scheduler:
  name: cosine_with_warmup
  t_warmup: 100ba
  t_max: 2ep
  alpha_f: 0.1
  scale_warmup: false

# ---- FSDP sharding ----
fsdp_config:
  sharding_strategy: FULL_SHARD        # shards model params, grads & opt-state
  mixed_precision: DEFAULT
  activation_checkpointing: true      # save memory by recomputing activations
  limit_all_gathers: true             # reduce all-gather overhead
  cpu_offload: false                  # keep shards on GPU (faster)
  verbose: true                       # logs “Using FSDP FULL_SHARD”

# ---- logging / callbacks ----
loggers:
  wandb:
    project: llm-foundry-demo
    entity: kannansarat9
callbacks:
  speed_monitor: {}
  lr_monitor: {}
  hf_checkpointer:
    save_folder: ./checkpoints              # where to write HF-format checkpoints
    save_interval: 1ep                      # every epoch
    huggingface_folder_name: "ba{batch}"      # sub-folder format
    precision: float16
    overwrite: true
    mlflow_registered_model_name: null      # no MLflow registry
    mlflow_logging_config: null             # defaults only
    flatten_imports: ["llmfoundry"]

eval_interval: 100ba
save_interval: 0          # disable checkpoints on Kaggle unless you need them
