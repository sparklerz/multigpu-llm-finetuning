# ---- experiment identity ----
run_name: mpt7b_dolly_sft
seed: ${variables.global_seed}    # this is the recognized field

# ---- any free-form variables go here ----
variables:
  global_seed: 42
  grad_accum: 4

device_train_microbatch_size: 1
global_train_batch_size: 8
max_seq_len: 1024
precision: bf16

# ---- model ----
model:
  name: hf_causal_lm
  pretrained_model_name_or_path: mosaicml/mpt-7b
  # keeps weights on CPU until ZeRO places them on GPUs
  init_device: meta
  tokenizer_name: mosaicml/mpt-7b
  config_overrides:
    max_seq_len: 1024

# ---- tokenizer ----
tokenizer:
  name: mosaicml/mpt-7b
  pretrained_model_name_or_path: mosaicml/mpt-7b
  init_device: cpu

# ---- data ----
train_loader:
  name: finetuning
  dataset:
    dataset_name: databricks/databricks-dolly-15k
    split: train
    prompt_column: instruction
    response_column: response
    shuffle: true
    max_seq_len: 1024
  drop_last: false
  device_batch_size: 1
  num_workers: 2
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  timeout: 0

eval_loader:
  name: finetuning
  dataset:
    dataset_name: databricks/databricks-dolly-15k
    split: validation
    prompt_column: instruction
    response_column: response
    shuffle: false
    max_seq_len: 1024
  drop_last: false
  device_batch_size: 1
  num_workers: 2
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  timeout: 0

# ---- optimisation ----
max_duration: 2ep
optimizer:
  name: decoupled_adamw
  lr: 1e-5
  betas: [0.9, 0.95]
  weight_decay: 0.05

scheduler:
  name: cosine_with_warmup
  t_warmup: 100ba
  t_max: 1000ba
  alpha_f: 0.1
  scale_warmup: false

# ---- FSDP sharding ----
fsdp_config:
  sharding_strategy: FULL_SHARD        # shards model params, grads & opt-state
  mixed_precision: FULL               # runs in bf16 everywhere
  activation_checkpointing: true      # save memory by recomputing activations
  limit_all_gathers: true             # reduce all-gather overhead
  cpu_offload: false                  # keep shards on GPU (faster)
  verbose: true                       # logs “Using FSDP FULL_SHARD”

# ---- logging / callbacks ----
loggers:
  wandb:
    project: llm-foundry-demo
    entity: WANDB_USERNAME
callbacks:
  speed_monitor: {}
  lr_monitor: {}
  hf_checkpointer:
    save_folder: ./checkpoints              # where to write HF-format checkpoints
    save_interval: 1ep                      # every epoch
    huggingface_folder_name: ba{batch}      # sub-folder format
    precision: float16
    overwrite: true
    mlflow_registered_model_name: null      # no MLflow registry
    mlflow_logging_config: null             # defaults only
    flatten_imports: ["llmfoundry"]

eval_interval: 100ba
save_interval: 0          # disable checkpoints on Kaggle unless you need them
