# ---- experiment identity ----
run_name: mpt7b_dolly_sft
seed: ${variables.global_seed}    # this is the recognized field

# ---- any free-form variables go here ----
variables:
  global_seed: 42
  grad_accum: 4
  lr_scheduler:
    name: cosine
    t_max: 1000ba
    eta_min: 1.0e-6

device_train_microbatch_size: 1
global_train_batch_size: 8
max_seq_len: 1024
precision: bf16

# ---- model ----
model:
  name: hf_causal_lm
  pretrained_model_name_or_path: mosaicml/mpt-7b
  init_device: meta
  tokenizer_name: mosaicml/mpt-7b
  config_overrides:
    max_seq_len: 1024

# ---- data ----
train_loader:
  name: jsonl_sft
  jsonl_dir: data/dolly_15k_json
  split: train
  shuffle: true
  batch_size: ${device_train_microbatch_size}
  num_workers: 2

eval_loader:
  name: jsonl_sft
  jsonl_dir: data/dolly_15k_json
  split: validation
  batch_size: ${device_train_microbatch_size}
  num_workers: 2

# ---- optimisation ----
max_duration: 2ep
optimizer:
  name: decoupled_adamw
  lr: 1e-5
  betas: [0.9, 0.95]
  weight_decay: 0.05

# ---- FSDP sharding ----
fsdp_config:
  sharding_strategy: FULL_SHARD
  mixed_precision: FULL
  activation_checkpointing: true
  limit_all_gathers: true
  cpu_offload: false
  verbose: true

# ---- logging / callbacks ----
loggers:
  wandb:
    project: llm-foundry-demo
    entity: WANDB_USERNAME
callbacks:
  speed_monitor: {}
  lr_monitor: {}
  huggingface_hub:
    repo_id: ash001/llm-foundry-fsdp-mpt-7B
    use_auth_token: true
    commit_message: "fsdp fine-tuned mpt-7b"
    save_strategy: epoch
    push_to_hub: true
    create_pr: false

eval_interval: 100ba
save_interval: 0