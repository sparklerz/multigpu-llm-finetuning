import os, math, torch, ray, wandb, pathlib, tempfile, shutil
from ray import tune, train
from ray.tune.schedulers import ASHAScheduler
from ray.train import Checkpoint
from ray.air import session, RunConfig, CheckpointConfig
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM, AutoTokenizer,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling,
    set_seed
)

os.environ.setdefault("WANDB_PROJECT", "ray-tune-qwen")
os.environ.setdefault("WANDB_WATCH",   "false")          # skip parameter histograms

MODEL = "Qwen/Qwen2-0.5B-Instruct"

# 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helper: load + tokenise IMDb once per trial
def get_imdb(tokenizer):
    ds = load_dataset("imdb", split="train[:2%]")
    def tok_fn(ex):                     # truncate to fit GPU memory
        return tokenizer(ex["text"], truncation=True, max_length=256)
    tok = ds.map(tok_fn, batched=True, remove_columns=["text", "label"])
    split = tok.train_test_split(test_size=0.1, seed=42)
    return split["train"], split["test"]

# 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Trial trainable
def train_fn(config):
    """Single Ray Tune trial â†’ fine-tunes the model + reports val-loss."""
    # Reproducibility
    set_seed(config.get("seed", 42))

    model_name = config.get(
        "model_name",
        MODEL
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    train_ds, val_ds = get_imdb(tokenizer)
    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

    run_name = f"tune-{session.get_trial_id()}"
    trial_dir = session.get_trial_dir()

    wandb.init(
        project=os.getenv("WANDB_PROJECT"),
        name=run_name,
        config=config,
        reinit=True,
    )

    args = TrainingArguments(
        output_dir="/tmp/ignore",
        run_name=run_name,
        report_to=["wandb"],
        per_device_train_batch_size=config["batch_size"],
        per_device_eval_batch_size=config["batch_size"],
        gradient_accumulation_steps=max(1, 32 // config["batch_size"]),
        gradient_checkpointing=True,
        learning_rate=config["lr"],
        weight_decay=config["weight_decay"],
        num_train_epochs=1,
        warmup_steps=config["warmup_steps"],
        eval_strategy="epoch",
        save_strategy="no",
        save_total_limit=1,
        logging_strategy="steps",
        logging_steps=1,
        max_grad_norm=1.0,
        bf16=torch.cuda.is_bf16_supported(), # bf16 on ampere+
        fp16=not torch.cuda.is_bf16_supported(),
        fp16_full_eval=False,
        dataloader_pin_memory=True
    )

    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
    model.gradient_checkpointing_enable()
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        data_collator=collator,
    )

    try:
        # â”€â”€ run & report one epoch at a time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for epoch in range(int(config["epochs"])):
            trainer.train(resume_from_checkpoint=None)
            metrics = trainer.evaluate()

            # log to W&B
            wandb.log(
                {"eval_loss": metrics["eval_loss"], "epoch": epoch + 1}
            )
            checkpoint = None
            if train.get_context().get_world_rank() == 0:
                with tempfile.TemporaryDirectory() as tmp:
                    trainer.save_model(tmp)              # writes model + tokenizer
                    tokenizer.save_pretrained(tmp)
                    checkpoint = Checkpoint.from_directory(tmp)
            # report to Ray Tune (drives ASHA)
            session.report(
                {"eval_loss": metrics["eval_loss"],
                 "training_iteration": epoch + 1},
                checkpoint=checkpoint
            )
        model_dir = os.path.join(trial_dir, "model")
        trainer.save_model(model_dir)
        tokenizer.save_pretrained(model_dir)
    finally:
        wandb.finish()

# 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    ray.init()

    # â”€â”€ Hyper-parameter search space â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    param_space = {
        "lr": tune.loguniform(5e-6, 5e-5),
        "batch_size": tune.choice([1, 2, 4]),
        "weight_decay": tune.uniform(0.0, 0.3),
        "epochs": tune.choice([1, 2, 3]),
        "warmup_steps": tune.choice([0, 100, 200]),
        "seed": 42,
        # Constant entries still live in config for transparency
        "model_name": MODEL,
    }

    # â”€â”€ Early-stopping scheduler (ASHA) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    scheduler = ASHAScheduler(
        time_attr="training_iteration",      # = epoch here
        metric="eval_loss",
        mode="min",
        max_t=3,                             # epochs
        grace_period=1,
        reduction_factor=2,
    )

    # â”€â”€ Build the tuner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tuner = tune.Tuner(
        tune.with_resources(
            train_fn,
            resources={"cpu": 2, "gpu": 1}  # â†’ 2 concurrent trials on 2 GPUs
        ),
        tune_config=tune.TuneConfig(
            scheduler=scheduler,
            num_samples=8,                   # total trials
        ),
        param_space=param_space,
        run_config=RunConfig(
            checkpoint_config=CheckpointConfig(
                num_to_keep=1,
                checkpoint_score_attribute="eval_loss",
                checkpoint_score_order="min",
            ),
        ),
    )

    results = tuner.fit()

    best = results.get_best_result(metric="eval_loss", mode="min")
    for r in results:
        if r.path != best.path:
            shutil.rmtree(r.path, ignore_errors=True)
    print("\nðŸŽ¯  Best hyper-params:", best.config)
    print("ðŸ“‰  Best eval-loss   :", best.metrics['eval_loss'])

    # â”€â”€  Push the best checkpoint to the Hub  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    with best.checkpoint.as_directory() as ckpt_dir:
        best_dir = pathlib.Path(ckpt_dir)

    from transformers import AutoModelForCausalLM, AutoTokenizer
    model = AutoModelForCausalLM.from_pretrained(best_dir)
    tokenizer = AutoTokenizer.from_pretrained(best_dir, use_fast=True, trust_remote_code=True)

    repo_id = "ash001/ray-tune-qwen-0.5B"
    model.push_to_hub(repo_id)
    tokenizer.push_to_hub(repo_id)

    print(f"âœ“ Pushed best model to https://huggingface.co/{repo_id}")